---
title: "Portfolio Exercise 3, Methods 3, 2021, autumn semester"
author: 'Alina Kereszt'
date: "13.12.2021"
output: pdf_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

**NB! The code contained within this portfolio was developed by the study group in collaboration (Mie Søgaard (M.S.), Linus Backström (L.B.), Mikkel Kristensen (M.K.), Alina Kereszt(A.K.)). The overall responsible individual for each section is indicated with initials in the beginning. Variables, code organization and explanations may vary between the study group members as they are individually created, as well as members may have additional material in their portfolio.**

# Exercises and objectives

1) Load the magnetoencephalographic recordings and do some initial plots to understand the data  
2) Do logistic regression to classify pairs of PAS-ratings  
3) Do a Support Vector Machine Classification on all four PAS-ratings  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This is Assignment 3 and will be part of your final portfolio   

```{python}
# load packages
import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import seaborn as sns
np.random.seed(420)
```


## EXERCISE 1 - Load the magnetoencephalographic recordings and do some initial plots to understand the data  (A.K.)

The files `megmag_data.npy` and `pas_vector.npy` can be downloaded here (http://laumollerandersen.org/data_methods_3/megmag_data.npy) and here (http://laumollerandersen.org/data_methods_3/pas_vector.npy)   

### 1) Load `megmag_data.npy` and call it `data` using `np.load`. You can use `join`, which can be imported from `os.path`, to create paths from different string segments
```{python}
# 'join' creates a handy, quick way to refer to a particular path, so you don't have to keep typing it out
# path = '~/GitHub/Coding/Methods 3/week_08'
# path = os.path.join(path, 'megmag_data.npy')

data = np.load('megmag_data.npy') # load data
```

**i. The data is a 3-dimensional array. The first dimension is number of repetitions of a visual stimulus , the second dimension is the number of sensors that record magnetic fields (in Tesla) that stem from neurons activating in the brain, and the third dimension is the number of time samples. How many repetitions, sensors and time samples are there?** 
    ```{python}
data.shape # show shape of data; it is a téglatest!!!
# BASICALLY what you have is a big cube of values that are ONLY THE MEASUREMENTS. So far, the variables 'repetition', 'sensor' and 'timestamp' are only implied - they have not been created.
```

There are 682 repetitions, 102 sensors and 251 timestamps.

**ii. The time range is from (and including) -200 ms to (and including) 800 ms with a sample recorded every 4 ms. At time 0, the visual stimulus was briefly presented. Create a 1-dimensional array called `times` that represents this.**
    ```{python}
# In Python, you can refer to specific components of an element with square brackets ([])
# Specifying '0' means you want to extract the first element of that variable
# A colon (:) means you want everything
# You can also specify a specific row to start or end with, e.g. [:7] will take everything until the 8th variable
times = np.arange(-200, 804, 4) 
```

**iii. Create the sensor covariance matrix $\Sigma_{XX}$: $$\Sigma_{XX} = \frac 1 N \sum_{i=1}^N XX^T$$ $N$ is the number of repetitions and $X$ has $s$ rows and $t$ columns (sensors and time), thus the shape is $X_{s\times t}$. Do the sensors pick up independent signals? (Use `plt.imshow` to plot the sensor covariance matrix)**
    ```{python}
# There are 102 sensors, so the covariance matrix is going to be 102 by 102
# To multiply an m×n matrix by an n×p matrix, the ns must be the same, and the result is an m×p matrix
# Here we multiply matrices 102x251 and 251x102 in size
# 'np.zeros' just creates an empty matrix to receive values
mat_1_1a = np.zeros(shape = (102, 102))
# Since our data is basically matrices stacked on top of each other, we can loop through each repetition and apply the formula
for i in range(0, 681):
  mat_1_1a += data[i, :, :] @ data[i, :, :].T
covmat_1_1a = mat_1_1a/682

plt.imshow(covmat_1_1a)
plt.colorbar()
plt.show()
```

As seen by the activation on the off-diagonal, there is some degree of covariance between the readings of the sensors, meaning their signals aren't entirely independent of each other.

**iv. Make an average over the repetition dimension using `np.mean` - use the `axis` argument. (The resulting array should have two dimensions with time as the first and magnetic field as the second)**
    ```{python}
rep_means = np.mean(data, axis = 0).T # transposed since repetition is the 1st dimension
```
**v. Plot the magnetic field (based on the average) as it evolves over time for each of the sensors (a line for each) (time on the x-axis and magnetic field on the y-axis). Add a horizontal line at $y = 0$ and a vertical line at $x = 0$ using `plt.axvline` and `plt.axhline`**
    ```{python}
# test = rep_means[0, :]
# plt.figure()
# plt.plot(times, test, 'b-')
# plt.show()

plt.figure()
# for i in range(102):
#  sensor_values = rep_means[:, i]
#  plt.plot(timestamps, sensor_values, 'm-', linewidth = 0.5)
plt.plot(times, rep_means)
plt.axvline(x = 0, color = 'k', linewidth = 0.5)
plt.axhline(y = 0, color = 'k', linewidth = 0.5)
plt.xlabel('Time (ms)')
plt.ylabel('Magnetic field (T)')
plt.title('Magnetic field by sensor through time')
plt.show()

# hehe a doodle
# plt.figure()
# plt.plot(rep_means[1], rep_means[0], 'b-')
# plt.show()
```

**vi. Find the maximal magnetic field in the average. Then use `np.argmax` and `np.unravel_index` to find the sensor that has the maximal magnetic field.**
    ```{python}
# Find the highest value:
# max_mag_field = np.amax(rep_means) # just out of curiosity

# Computers store info in a linear way. Each memory cell corresponds to a number. A block of memory can be addressed in terms of a base, which is the memory address of its first element, and the item index.
# Like how Arabic numbers have a base of 10. Take an array of numbers 10x7, whole numbers starting from 1. If we look for the linear index 27 (which in this case happens to contain the number 27), we would find that it is in the 3rd row, and in the seventh column, so its 2D index would be (2, 7).
# This function finds the linear index of the value we found for our maximal magnetic field:
linear_index = np.argmax(rep_means)
# And this one handily calculates the (in this case) 2D index of it - the second number of which will be our sensor!
sensor_2d_index = np.unravel_index(linear_index, (251, 102))
sensor_2d_index
# It's sensor 73! For timestamp 112
```
**vii. Plot the magnetic field for each of the repetitions (a line for each) for the sensor that has the maximal magnetic field. Highlight the time point with the maximal magnetic field in the average (as found in 1.1.v) using `plt.axvline`**  
    ```{python}
# Isolate the data for sensor 73
time_pos = sensor_2d_index[0]
sensor_pos = sensor_2d_index[1]
sensor73 = data[:, sensor_pos, :]

plt.figure()
plt.plot(times, rep_means[:, sensor_pos]. color = 'm')
plt.axvline(x = 0, color = 'k', linewidth = 0.5)
plt.axhline(y = 0, color = 'k', linewidth = 0.5)
plt.xlabel('Time (ms)')
plt.ylabel('Magnetic field (T)')
plt.title('Average magnetic field of sensor 73 through time')
plt.show()

plt.figure()
plt.plot(times, sensor73.T, linewidth = 0.1)
plt.axvline(x = times[time_pos], color = 'g', linewidth = 0.5)
plt.axvline(x = 0, color = 'k', linewidth = 0.5)
plt.axhline(y = 0, color = 'k', linewidth = 0.5)
plt.xlabel('Time (ms)')
plt.ylabel('Magnetic field (T)')
plt.title('Magnetic field for sensor 73 through time by repetition')
plt.show()
```
**viii. Describe in your own words how the response found in the average is represented in the single repetitions. But do make sure to use the concepts _signal_ and _noise_ and comment on any differences on the range of values on the y-axis**

When taking the averages for each sensor, we are actually plotting a fair amount of noise since each sensor takes readings from a slightly different area of the brain; i.e. we use also signals from areas that may not even be associated with the studied phenomenon. This is clearly showed in the plot of averages by the peaks of different sensors often reaching in opposite directions. These averages can nevertheless be used to find the activation areas in the brain associated with the phenomenon we are studying, and to filter out the sensors that are producing readings that are irrelevant to us.

The aggregated shape of the individual repetitions does indeed seem to approximate the averaged readings from the sensor. However, the readings are nevertheless spread out along the y axis indicating a large amount of noise even in the individual readings. It could even be argued that the peak itself emerges more because the lower bound of the readings is relatively high, rather than because the upper bound is high.

### 2) Now load `pas_vector.npy` (call it `y`). PAS is the same as in Assignment 2, describing the clarity of the subjective experience the subject reported after seeing the briefly presented stimulus  
```{python}
y = np.load('pas_vector.npy') # load data
```

**i. Which dimension in the `data` array does it have the same length as?**
```{python}
data.shape
y.shape # same length as the number of repetitions, i.e. one PAS-rating is given for each repetition
```

The two arrays are matched along the first axis of _data_, i.e. one PAS-rating is given for each repetition.

**ii. Now make four averages (As in Exercise 1.1.iii), one for each PAS rating, and plot the four time courses (one for each PAS rating) for the sensor found in Exercise 1.1.v**
```{python}
pas1 = data[np.where(y == 1)]
pas2 = data[np.where(y == 2)]
pas3 = data[np.where(y == 3)]
pas4 = data[np.where(y == 4)] # slice away! since the lengths match, it just assumes that y is laid along the axis of data that matches its length

# Find mean
pas1_mean = np.mean(pas1, axis = 0)
pas2_mean = np.mean(pas2, axis = 0)
pas3_mean = np.mean(pas3, axis = 0)
pas4_mean = np.mean(pas4, axis = 0)

plt.figure()
plt.plot(times, pas1_mean[sensor_pos,], 'm-', linewidth = 0.5)
plt.plot(times, pas2_mean[sensor_pos,], 'c-', linewidth = 0.5)
plt.plot(times, pas3_mean[sensor_pos,], 'r-', linewidth = 0.5)
plt.plot(times, pas4_mean[sensor_pos,], 'b-', linewidth = 0.5)
plt.axvline(x = 0, color = 'k', linewidth = 0.5)
plt.axvline(x = times[time_pos], color = 'g', linewidth = 0.5)
plt.axhline(y = 0, color = 'k', linewidth = 0.5)
plt.xlabel('Time (ms)')
plt.ylabel('Magnetic field (T)')
plt.title('Magnetic field for Sensor 73 through time by PAS-rating')
plt.legend(['PAS1', 'PAS2', 'PAS3', 'PAS4'])
plt.show()

# paska, perse, voi vittu, perkele
```

**iii. Notice that there are two early peaks (measuring visual activity from the brain), one before 200 ms and one around 250 ms. Describe how the amplitudes of responses are related to the four PAS-scores. Does PAS 2 behave differently than expected?**

When participants rated the trial as PAS 1, it meant that didn't perceive it. As such, since the brain might not even have processed the stimulus, it makes sense that the peaks are small compared to the other PAS-ratings.

PAS 2, where the participant reported barely perceiving stimulus, behaves surprisingly. It peaks similarly to PAS 3 and 4, indicating that despite the conscious recollection from the participant, the stimulus may in fact have gotten processed subconsciously. In fact, PAS 2 has the largest amplitude at the second peak; however, whether this is significant cannot be deduced from the plot.

## EXERCISE 2 - Do logistic regression to classify pairs of PAS-ratings  (M.K., L.B.)

### 1) Now, we are going to do Logistic Regression with the aim of classifying the PAS-rating given by the subject (M.K.)

**i. We'll start with a binary problem - create a new array called `data_1_2` that only contains PAS responses 1 and 2. Similarly, create a `y_1_2` for the target vector**
```{python}
data_1_2 = data[np.where((y == 1) | (y == 2))]
y_1_2 = y[np.where((y == 1) | (y == 2))]
```

**ii. Scikit-learn expects our observations (`data_1_2`) to be in a 2d-array, which has samples (repetitions) on dimension 1 and features (predictor variables) on dimension 2. Our `data_1_2` is a three-dimensional array. Our strategy will be to collapse our two last dimensions (sensors and time) into one dimension, while keeping the first dimension as it is (repetitions). Use `np.reshape` to create a variable `X_1_2` that fulfils these criteria.**
```{python}
shape = data_1_2.shape
time_by_sensor = shape[1] * shape[2]
X_1_2 = np.reshape(data_1_2, (shape[0], time_by_sensor)) # 1st dimension intact, 2nd and 3rd collapsed into single dimension equal to the product of their length
```

**iii. Import the `StandardScaler` and scale `X_1_2`**  
```{python}
sc = StandardScaler() # assign a class to an object so it can take on a function and possess certain attributes
X_1_2_std = sc.fit_transform(X_1_2) # standardize the data through gradient descent; it should have mean 0 and standardized format
```

**iv. Do a standard `LogisticRegression` - can be imported from `sklearn.linear_model` - make sure there is no `penalty` applied**
```{python}
# don't we need to split the dataset into train and test lol
logR = LogisticRegression(penalty = 'none') # assign a class to an object so it can take on a function and possess certain attributes
model_2_1a = logR.fit(X_1_2_std, y_1_2) # fit the model based on scaled X and y
# This is essentially a logistic regression with 25602 coefficients/weights - we have that many variables
# model_2_1a.coef_[0, 1212] # the model is a nested list; you take list number 0, and item number 1212
# the higher the C the less penalty
```

**v. Use the `score` method of `LogisticRegression` to find out how many labels were classified correctly. Are we overfitting? Besides the score, what would make you suspect that we are overfitting?** 
```{python}
model_2_1a.score(X_1_2_std, y_1_2) # gives value between 0 or 1
```

The output of '1' means that all the labels (100%) were classified correctly, indicating overfitting. This is unsurprising: we are validating the model in-sample, i.e. on the exact same data as that which we trained the model on, and neither are we penalizing the model for complexity in any way.

**vi. Now apply the _L1_ penalty instead - how many of the coefficients (`.coef_`) are non-zero after this?**
```{python}
logR = LogisticRegression(penalty = 'l1', solver = 'liblinear') # assign function to object
# The following line throws an error with the default solver (which is the algorithm used in the optimization problem). Documentation suggests that we can use either 'liblinear' or 'saga'. I choose 'liblinear' because 'saga' doesn't converge
model_2_1b = logR.fit(X_1_2_std, y_1_2) # fit the model based on scaled X and y

# Flatten the matrix into a vector
coef_vector = model_2_1b.coef_.flatten()
# Find number of those that are non-zero
non_zero = coef_vector != 0 #take elements of the vector that are not equal to zero
# X_1_2 is (214, 25602)
# We want a matrix that has number of reps as one axis and non-zero coefs as the other axis
X_1_2_nz = X_1_2_std[:, non_zero]
X_1_2_nz.shape
```

We now have 280 non-zero coefficients.

**vii. Create a new reduced $X$ that only includes the non-zero coefficients - show the covariance of the non-zero features (two covariance matrices can be made; $X_{reduced}X_{reduced}^T$ or $X_{reduced}^TX_{reduced}$ (you choose the right one)) . Plot the covariance of the features using `plt.imshow`. Compared to the plot from 1.1.iii, do we see less covariance?**
```{python}
# We're checking the covariance of 285 variables, i.e. we want a matrix that is (285, 285), so for this we need to multiply matrices (285, 214) and (214, 285) in dimensions
covmat_2_1a = X_1_2_nz.T @ X_1_2_nz
plt.figure()
plt.imshow(covmat_2_1a)
plt.colorbar()
plt.show()
```

Compared to the previous covariance matrix, there appears to be less covariance. This makes sense, as we have removed a large amount of variables that weren't explaining additional variance.

### 2) Now, we are going to build better (more predictive) models by using cross-validation as an outcome measure (L.B.)  
**i. Import `cross_val_score` and `StratifiedKFold` from `sklearn.model_selection`** 

**ii. To make sure that our training data sets are not biased to one target (PAS) or the other, create `y_1_2_equal`, which should have an equal number of each target. Create a similar `X_1_2_equal`. The function `equalize_targets_binary` in the code chunk associated with Exercise 2.2.ii can be used. Remember to scale `X_1_2_equal`!**
    ```{python}
# Function:
def equalize_targets_binary(data, y):
    np.random.seed(420)
    targets = np.unique(y) ## find the number of targets, i.e. possible values y can take on
    if len(targets) > 2:
        raise NameError("can't have more than two targets")
    counts = list() #initialize list
    indices = list() #initialize list
    for target in targets:
        counts.append(np.sum(y == target)) # 
        indices.append(np.where(y == target)[0]) ## find their indices
    min_count = np.min(counts)
    # randomly choose trials
    first_choice = np.random.choice(indices[0], size=min_count, replace=False)
    second_choice = np.random.choice(indices[1], size=min_count,replace=False)
    
    # create the new data sets
    new_indices = np.concatenate((first_choice, second_choice))
    new_y = y[new_indices]
    new_data = data[new_indices, :, :]
    
    return new_data, new_y
  
equalized = equalize_targets_binary(data_1_2, y_1_2)
X_1_2_equal = equalized[0]
y_1_2_equal = equalized[1]

# REshape data
X_1_2_equal_2 = np.reshape(X_1_2_equal, newshape = (198, 25602))
X_1_2_equal_std = sc.fit_transform(X_1_2_equal_2)
```

**iii. Do cross-validation with 5 stratified folds doing standard `LogisticRegression` (See Exercise 2.1.iv)** 
    ```{python}
model_2_2a = LogisticRegression(penalty = 'none').fit(X_1_2_equal_std, y_1_2_equal)
cv = StratifiedKFold(n_splits = 5)
score_2_2a = cross_val_score(model_2_2a, X_1_2_equal_std, y_1_2_equal, cv = cv)
np.mean(score_2_2a)
```
**iv. Do L2-regularisation with the following `Cs=  [1e5, 1e1, 1e-5]`. Use the same kind of cross-validation as in Exercise 2.2.iii. In the best-scoring of these models, how many more/fewer predictions are correct (on average)?**
    ```{python}
logR1 = LogisticRegression(penalty = 'l2', solver = 'lbfgs', C = 1e5)
logR2 = LogisticRegression(penalty = 'l2', solver = 'lbfgs', C = 1e1)
logR3 = LogisticRegression(penalty = 'l2', solver = 'lbfgs', C = 1e-5)

model_2_2b = logR1.fit(X_1_2_equal_std, y_1_2_equal)
model_2_2c = logR2.fit(X_1_2_equal_std, y_1_2_equal)
model_2_2d = logR3.fit(X_1_2_equal_std, y_1_2_equal)

score_2_2b = cross_val_score(model_2_2b, X_1_2_equal_std, y_1_2_equal, cv = cv)
score_2_2c = cross_val_score(model_2_2c, X_1_2_equal_std, y_1_2_equal, cv = cv)
score_2_2d = cross_val_score(model_2_2d, X_1_2_equal_std, y_1_2_equal, cv = cv)

np.mean(score_2_2b)
np.mean(score_2_2c)
np.mean(score_2_2d)
```

It seems that the model with the highest C-value, 1e5, is the best-scoring model - it achieves the same accuracy as the previous model.

**v. Instead of fitting a model on all `n_sensors * n_samples` features, fit  a logistic regression (same kind as in Exercise 2.2.iv (use the `C` that resulted in the best prediction)) for __each__ time sample and use the same cross-validation as in Exercise 2.2.iii. What are the time points where classification is best? Make a plot with time on the x-axis and classification score on the y-axis with a horizontal line at the chance level (what is the chance level for this analysis?)**
    ```{python}
logR_v = LogisticRegression(penalty = 'l2', solver = 'lbfgs', C = 1e5)
score_means = np.zeros(shape = (251)) # create empty variable to receive scores
for i in range(251):
  X_1_2_bytime = X_1_2_equal[:, :, i]
  X_1_2_bytime = sc.fit_transform(X_1_2_bytime)
  logR_v.fit(X_1_2_bytime, y_1_2_equal)
  score = cross_val_score(logR_v, X_1_2_bytime, y_1_2_equal, cv = cv)
  score_mean = np.mean(score)
  score_means[i] = score_mean
  
highest_score = np.argmax(score_means)
np.amax(score_means)
times[highest_score]

plt.figure()
plt.plot(times, score_means)
plt.axvline(x = times[highest_score], color = 'r')
plt.axhline(y = 0.50, color = 'k', linestyle = ':')
plt.title('Classification accuracy by timestamp')
plt.show()
```
 
The highest classification score is 0.6262 at 228 ms.
 
**vi. Now do the same, but with L1 regression - set `C=1e-1` - what are the time points when classification is best? (make a plot)?**
    ```{python}
logR_vi = LogisticRegression(penalty = 'l1', C = 1e-1, solver = 'liblinear')
score_means_vi = np.zeros(shape = (251)) # create empty variable to receive scores
for i in range(251):
  X_1_2_bytime = X_1_2_equal[:, :, i]
  X_1_2_bytime = sc.fit_transform(X_1_2_bytime)
  logR_vi.fit(X_1_2_bytime, y_1_2_equal)
  score = cross_val_score(logR_vi, X_1_2_bytime, y_1_2_equal, cv = cv)
  score_mean = np.mean(score)
  score_means_vi[i] = score_mean
  
highest_score_vi = np.argmax(score_means_vi)
np.amax(score_means_vi)
times[highest_score_vi]

plt.figure()
plt.plot(times, score_means_vi)
plt.axvline(x = times[highest_score_vi], color = 'r')
plt.axhline(y = 0.50, color = 'k', linestyle = ':')
plt.title('Classification accuracy by timestamp')
plt.show()
```

The highest classification score is 0.6464 at 236 ms.

**vii. Finally, fit the same models as in Exercise 2.2.vi but now for `data_1_4` and `y_1_4` (create a data set and a target vector that only contains PAS responses 1 and 4). What are the time points when classification is best? Make a plot with time on the x-axis and classification score on the y-axis with a horizontal line at the chance level (what is the chance level for this analysis?)**
```{python}
# Equalize targets
data_1_4 = data[np.where((y == 1) | (y == 4))]
y_1_4 = y[np.where((y == 1) | (y == 4))]

equalized_1_4 = equalize_targets_binary(data_1_4, y_1_4)
X_1_4_equal = equalized_1_4[0]
y_1_4_equal = equalized_1_4[1]

# Run model
logR_vii = LogisticRegression(penalty = 'l1', C = 1e-1, solver = 'liblinear')
score_means_vii = np.zeros(shape = (251)) # create empty variable to receive scores
for i in range(251):
  X_1_4_bytime = X_1_4_equal[:, :, i]
  X_1_4_bytime = sc.fit_transform(X_1_4_bytime)
  logR_vii.fit(X_1_4_bytime, y_1_4_equal)
  score = cross_val_score(logR_vii, X_1_4_bytime, y_1_4_equal, cv = cv)
  score_mean = np.mean(score)
  score_means_vii[i] = score_mean
  
highest_score_vii = np.argmax(score_means_vii)
np.amax(score_means_vii)
times[highest_score_vii]

# Plot
plt.figure()
plt.plot(times, score_means_vii)
plt.axvline(x = times[highest_score_vii], color = 'r')
plt.axhline(y = 0.50, color = 'k', linestyle = ':')
plt.title('Classification accuracy by timestamp')
plt.show()
```

The highest classification score is 0.6913 at 228 ms.

### 3) Is pairwise classification of subjective experience possible? Any surprises in the classification accuracies, i.e. how does the classification score fore PAS 1 vs 4 compare to the classification score for PAS 1 vs 2?  
    Classification for PAS 1 and 4 is much better than for PAS 1 and 2: more scores are above chance level. This was to be expected as PAS 4 indicates a subjective experience that was fully perceived, while a PAS 2 indicates a subjective experience that was barely perceived, compared to PAS 1 not being perceived at all. There is a greater difference between the subjective experiences of PAS 1 and 4 than between PAS 1 and 2, and this is reflected in the accuracy scores.

## EXERCISE 3 - Do a Support Vector Machine Classification on all four PAS-ratings (M.S.)
### 1) Do a Support Vector Machine Classification  
**i. First equalize the number of targets using the function associated with each PAS-rating using the function associated with Exercise 3.1.i**
```{python}
def equalize_targets(data, y):
    np.random.seed(7)
    targets = np.unique(y)
    counts = list()
    indices = list()
    for target in targets:
        counts.append(np.sum(y == target))
        indices.append(np.where(y == target)[0])
    min_count = np.min(counts)
    first_choice = np.random.choice(indices[0], size=min_count, replace=False)
    second_choice = np.random.choice(indices[1], size=min_count, replace=False)
    third_choice = np.random.choice(indices[2], size=min_count, replace=False)
    fourth_choice = np.random.choice(indices[3], size=min_count, replace=False)
    
    new_indices = np.concatenate((first_choice, second_choice,
                                 third_choice, fourth_choice))
    new_y = y[new_indices]
    new_data = data[new_indices, :, :]
    
    return new_data, new_y
  
equalized2 = equalize_targets(data, y)
X_all_equal = equalized2[0]
y_all_equal = equalized2[1]
```
**ii. Run two classifiers, one with a linear kernel and one with a radial basis (other options should be left at their defaults) - the number of features is the number of sensors multiplied the number of samples. Which one is better predicting the category?**
```{python}
# Reshape data
shape2 = X_all_equal.shape
time_by_sensor = shape2[1] * shape2[2]
X_all = np.reshape(X_all_equal, (shape2[0], time_by_sensor))

#SVM setup
cv = StratifiedKFold()
X = X_all
y = y_all_equal

# Scale data
sc = StandardScaler()
sc.fit(X)
X_all_std = sc.transform(X)

svm_linear = SVC(kernel = 'linear')
scores_svm_linear = cross_val_score(svm_linear, X_all_std, y, cv = cv)

svm_rbf = SVC(kernel = 'rbf') #radial basis function
scores_svm_rbf = cross_val_score(svm_rbf, X_all_std, y, cv = cv)

np.mean(scores_svm_linear)
np.mean(scores_svm_rbf)
```

The Support Vector Machine with the radial basis kernel is better at predicting the category.

**iii. Run the sample-by-sample analysis (similar to Exercise 2.2.v) with the best kernel (from Exercise 3.1.ii). Make a plot with time on the x-axis and classification score on the y-axis with a horizontal line at the chance level (what is the chance level for this analysis?)**
```{python}
svm_rbf = SVC(kernel = 'rbf') #radial basis function
score_means = np.zeros(shape = (251)) # create empty variable to receive scores
for i in range(251):
  X_all_bytime = X_all_equal[:, :, i]
  X_all_bytime = sc.fit_transform(X_all_bytime)
  svm_rbf.fit(X_all_bytime, y_all_equal)
  score = cross_val_score(svm_rbf, X_all_bytime, y_all_equal, cv = cv)
  score_mean = np.mean(score)
  score_means[i] = score_mean
  
plt.figure()
plt.plot(times, score_means)
plt.axhline(y = 0.25, color = 'k', linestyle = ':')
plt.title('Classification accuracy by timestamp')
plt.show()
```
**iv. Is classification of subjective experience possible at around 200-250 ms?**
The classifier always performs better than chance at 200-250 ms; however, it does not seem to perform better than about 35% accuracy, which, in the real world, would hardly constitute a successful classifier.


### 2) Finally, split the equalized data set (with all four ratings) into a training part and test part, where the test part if 30 % of the trials. Use `train_test_split` from `sklearn.model_selection`  
```{python}
X_train, X_test, y_train, y_test = train_test_split(X_all, y_all_equal, test_size = 0.3,
                                                    random_state = 0)
```
**i. Use the kernel that resulted in the best classification in Exercise 3.1.ii and `fit`the training set and `predict` on the test set. This time your features are the number of sensors multiplied by the number of samples.**
```{python}
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
svm_rbf = SVC(kernel = 'rbf') #radial basis function
svm_rbf_fit = svm_rbf.fit(X_train_std, y_train)
predictions = svm_rbf.predict(X_test_std)
```
    ii. Create a _confusion matrix_. It is a 4x4 matrix. The row names and the column names are the PAS-scores. There will thus be 16 entries. The PAS1xPAS1 entry will be the number of actual PAS1, $y_{pas1}$ that were predicted as PAS1, $\hat y_{pas1}$. The PAS1xPAS2 entry will be the number of actual PAS1, $y_{pas1}$ that were predicted as PAS2, $\hat y_{pas2}$ and so on for the remaining 14 entries.  Plot the matrix
    ```{python}
cm = confusion_matrix(y_test, predictions)

# 'feature scaling' is just fancy for normalizing and standardizing
plt.figure()
heatmap = sns.heatmap(cm.T, annot = True, cmap = 'PuBuGn')
heatmap.set_title('Actual vs predicted PAS-rating')
heatmap.set_xlabel('Actual PAS')
heatmap.set_ylabel('Predicted PAS')
plt.show()
```
**iii. Based on the confusion matrix, describe how ratings are misclassified and if that makes sense given that ratings should measure the strength/quality of the subjective experience. Is the classifier biased towards specific ratings?**
Subjective experiences are most commonly misclassified as being PAS 3, i.e. the classifier is biased towards this rating. This would make sense, as earlier in this portfolio exercise, we had seen that the readings for the magnetic fields of the sensors did not much differ between PAS 2, 3 and 4, so the classifier might be defaulting to the middle value among three similar values.
    
    
    
    
    
    
    
    
    
    
    
    
    