---
title: "practical_exercise_3, Methods 3, 2021, autumn semester"
author: 'Alina Kereszt'
date: "29.09.2021"
output: html_document
---

<style type="text/css">
  body{
  font-size: 14pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/GitHub/Coding/Methods 3/week_03")
pacman::p_load(tidyverse, lme4, lmerTest, dfoptim, rsq, boot)
```

# Exercises and objectives
The objectives of the exercises of this assignment are:  
1) Download and organise the data and model and plot staircase responses based on fits of logistic functions  
2) Fit multilevel models for response times  
3) Fit multilevel models for count data  

REMEMBER: In your report, make sure to include code that can reproduce the answers requested in the exercises below (__MAKE A KNITTED VERSION__)  
REMEMBER: This assignment will be part of your final portfolio

## Exercise 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 2 (there should be 29).  
The data is associated with Experiment 2 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007  

1) Put the data from all subjects into a single data frame  
```{r, results = 'hide', warning = FALSE, error = FALSE, message = FALSE}
df <-
    list.files(pattern = "*.csv") %>% 
    map_df(~read_csv(.))
```

2) Describe the data and construct extra variables from the existing variables  
    i. add a variable to the data frame and call it _correct_ (have it be a _logical_ variable). Assign a 1 to each row where the subject indicated the correct answer and a 0 to each row where the subject indicated the incorrect answer (__Hint:__ the variable _obj.resp_ indicates whether the subject answered "even", _e_ or "odd", _o_, and the variable _target_type_ indicates what was actually presented.
```{r}
df$correct <- ifelse((df$target.type == "odd" & df$obj.resp == "o") | (df$target.type == "even" & df$obj.resp == "e"), 1, 0)
df$correct <- as.factor(df$correct) #encoded as "factor" because encoding as "logical" changes values to "TRUE" and "FALSE", which is not suitable for running and plotting logistic regression
```
    ii. describe what the following variables in the data frame contain, _trial.type_, _pas_, _trial_, _target.contrast_, _cue_, _task_, _target_type_, _rt.subj_, _rt.obj_, _obj.resp_, _subject_ and _correct_. (That means you can ignore the rest of the variables in your description). For each of them, indicate and argue for what `class` they should be classified into, e.g. _factor_, _numeric_ etc.  
      - _trial.type_: A staircase procedure was performed before the actual experiment procedure in order to find each participant's perceptual threshold. Since there are two categories of trial available, and they are not along a continuum, the variable should be encoded as a *factor*.
      - _pas_: The participant's experience on the PAS (Perceptual Awareness Scale), which categorizes how clearly they believe to have perceived the stimulus: No Experience (NE - 1), Weak Glimpse (WG - 2), Almost Clear Experience (ACE - 3) and Clear Experience (CE - 4). These are distinct categories and as such should be encoded as *factor*.
      - _trial_: The number of the trial at which the data was collected. Lies along a continuum and should therefore be coded as *numeric*.
      - _target.contrast_: The amount of contrast between the background and the stimulus. Lies along a continuum, so it should be *numeric*.
      - _cue_: The number code for the cue. There are 32 discrete possible combinations that do not lie along a continuum, and should therefore be encoded as a *factor*.
      - _task_: The amount of numbers that can be shown (singles = 1 even + 1 odd; pairs = 2 even + 2 odd; quadruplets; 4 even + 4 odd). It should be possible to analyze the data based on the different discrete categories of top-down expectations created, so it should be a *factor*. 
      - _target_type_: Whether the visual stimulus presented to the participant consisted of an even or an odd number. It should be possible to analyze the data based on the different discrete categories of stimuli available to the participant, so it should be a *factor*.
      - _rt.subj_: The amount of time it took the participant to decide where their experience fell on the PAS scale. Time lies along a continuum, and this variable should therefore be coded as *numeric*.
      - _rt.obj_: The amount of time it took the participant to make the judgment as to whether they had seen an even or an odd number. Time lies along a continuum, and this variable should therefore be coded as *numeric*.
      - _obj.resp_: The participant's response as to whether they had seen an even or an odd number. It should be possible to analyze the data based on the different categories of responses available to the participant, so it should be a *factor*.
      - _subject_: The ID for the participant completing the experiment. Because the participants' behavior and responses will depend highly on their individual cognition, it should be possible analyze the data while establishing a random intercept for each participant to avoid the aforementioned 'noise', and thereby code it as a *factor*.
      - _correct_: Whether the participant indicated the correct answer as to whether they had seen an even or an odd number. This should be *logical*, because the participant was either correct, or not - there is no continuum between these two possibilities. 
```{r}
#Encode them appropriately
df$trial <- as.numeric(df$trial)
df$trial.type <- as.factor(df$trial.type)
df$pas <- as.factor(df$pas)
df$target.contrast <- as.numeric(df$target.contrast)
df$cue <- as.factor(df$cue)
df$task <- as.factor(df$task)
df$target.type <- as.factor(df$target.type)
df$rt.subj <- as.numeric(df$rt.subj)
df$rt.obj <- as.numeric(df$rt.obj)
df$obj.resp <- as.factor(df$obj.resp)

#Anonymize subjects (also facilitates later analysis if the subjects are represented only by numbers)
df$subject <- as.numeric(df$subject)
df$subject <- as.factor(df$subject)
```
    iii. for the staircasing part __only__, create a plot for each subject where you plot the estimated function (on the _target.contrast_ range from 0-1) based on the fitted values of a model (use `glm`) that models _correct_ as dependent on _target.contrast_. These plots will be our _no-pooling_ model. Comment on the fits - do we have enough data to plot the logistic functions?
```{r}
staircase <- df %>% 
  filter(trial.type == "staircase") #isolate staircasing data

for (i in 1:29) {
  loopdf <- staircase %>%
    filter(subject == i) #isolate each subject since we are not organizing the data hierarchically, but rather analyzing everything with each participant entirely separate
  loopmodel <- glm(correct ~ target.contrast, data = loopdf, family = binomial) #run logistic regression on subject's data
  loopdf <- loopdf %>%
    mutate(inv = inv.logit(loopmodel$fitted.values)) #get the inverse logit of the fitted values
  plot <- ggplot(loopdf, aes(x = target.contrast, y = inv)) +
    geom_point(aes(color = "coral1")) +
    scale_color_manual(labels = c("No-pool function"), values = c("coral1")) +
    guides(color=guide_legend("Functions")) + 
    labs(title = "Estimated function for the participant", x = "Target contrast", y = "Probability of correct answer") #plot the thus estimated function
  print(plot) #actually get the plot to show up
}
```
    There does not appear to be sufficient data to plot the logistic functions: data points are sparse and the function is highly variable from participant to participant.
    iv. on top of those plots, add the estimated functions (on the _target.contrast_ range from 0-1) for each subject based on a partial pooling model (use `glmer` from the package `lme4`) where unique intercepts and slopes for _target.contrast_ are modelled for each _subject_ 
```{r}
model_1 <- lme4::glmer(correct ~ target.contrast + (1 | subject), family = binomial, data = staircase)
summary(model_1)
staircase <- staircase %>%
  mutate(pooled_inv = inv.logit(fitted.values(model_1)))

for (i in 1:29) {
  #same as in previous exercise
  loopdf <- staircase %>%
    filter(subject == i) 
  loopmodel <- glm(correct ~ target.contrast, data = loopdf, family = binomial) 
  loopdf <- loopdf %>%
    mutate(inv = inv.logit(loopmodel$fitted.values))
  #now for overlaying the pooled function
  plot <- ggplot(loopdf, aes(x = target.contrast, y = inv)) +
    geom_point(aes(y = inv, color = "coral1")) +
    geom_point(aes(y = pooled_inv, color = "cornflowerblue")) +
    scale_color_manual(labels = c("Partial-pooling function", "No-pool function"), values = c("cornflowerblue", "coral1")) +
    guides(color=guide_legend("Functions")) + 
    labs(title = "Estimated functions for the participant", x = "Target contrast", y = "Probability of correct answer") #plot the thus estimated function
  print(plot) #actually get the plot to show up
}
```
    v. in your own words, describe how the partial pooling model allows for a better fit for each subject 
    When the data is analyzed by taking each subject separately, only a small portion of the data is used (none of the other participants' data is included, despite them having taken part in the same experiment under the same conditions). When using a hierarchical model, all of the data is used in order to calculate the estimated function, with the different subjects simply recognized as having _slightly_ different curves that nevertheless resemble the overall curve, thereby allowing more general tendencies to be represented. Essentially, the hierarchical model recognizes both the differences and the similarities in the sample of participants.

## Exercise 2

Now we __only__ look at the _experiment_ trials (_trial.type_)
```{r}
exp <- df %>% 
  filter(trial.type == "experiment")
```
1) Pick four subjects and plot their Quantile-Quantile (Q-Q) plots for the residuals of their objective response times (_rt.obj_) based on a model where only intercept is modeled 
```{r}
# Isolate subjects
sub001 <- exp %>% filter(subject == "1")
sub005 <- exp %>% filter(subject == "5")
sub016 <- exp %>% filter(subject == "16")
sub020 <- exp %>% filter(subject == "20")

# Model only the intercept of their objective response times (basically the mean)
int001 <- lm(rt.obj ~ 1, data = sub001)
int005 <- lm(rt.obj ~ 1, data = sub005)
int016 <- lm(rt.obj ~ 1, data = sub016)
int020 <- lm(rt.obj ~ 1, data = sub020)

# Plot residuals with Q-Q plots
qqnorm(residuals(int001))
qqline(residuals(int001))

qqnorm(residuals(int005))
qqline(residuals(int005))

qqnorm(residuals(int016))
qqline(residuals(int016))

qqnorm(residuals(int020))
qqline(residuals(int020))

```
    i. comment on these
    The residuals are clearly not normally distributed (they do not approximate the line), and they are heavily skewed.
    ii. does a log-transformation of the response time data improve the Q-Q-plots?
```{r}
#Log-transform variables
sub001$log.rt.obj <- log(sub001$rt.obj)
sub005$log.rt.obj <- log(sub005$rt.obj)
sub016$log.rt.obj <- log(sub016$rt.obj)
sub020$log.rt.obj <- log(sub020$rt.obj)

# Model intercepts
log.int001 <- lm(log.rt.obj ~ 1, data = sub001)
log.int005 <- lm(log.rt.obj ~ 1, data = sub005)
log.int016 <- lm(log.rt.obj ~ 1, data = sub016)
log.int020 <- lm(log.rt.obj ~ 1, data = sub020)

# Plot residuals with Q-Q plots
qqnorm(residuals(log.int001))
qqline(residuals(log.int001))

qqnorm(residuals(log.int005))
qqline(residuals(log.int005))

qqnorm(residuals(log.int016))
qqline(residuals(log.int016))

qqnorm(residuals(log.int020))
qqline(residuals(log.int020))
```
    The residuals appear significantly more normal after log-transformation, being skewed to a much smaller extent.
2) Now do a partial pooling model modeling objective response times as dependent on _task_. (set `REML=FALSE` in your `lmer`-specification) 
```{r}
#Why REML = FALSE? 
#Itâ€™s generally good to use REML, if it is available, when you are interested in the magnitude of the random effects variances, but never when you are comparing models with different fixed effects via hypothesis tests or information-theoretic criteria such as AIC.

# Build models
model_2.1 <- lmerTest::lmer(rt.obj ~ task + (1 | subject), data = exp, REML = FALSE)
model_2.2 <- lmerTest::lmer(rt.obj ~ task + (1 + pas | subject), data = exp, REML = FALSE)

# Compare models
AIC(model_2.1, model_2.2) 

# Residual variance:
c(var(resid(model_2.1)),
  var(resid(model_2.2))
)

# Residual standard deviation
c(summary(model_2.1)$sigma,
  summary(model_2.2)$sigma
)
```
    i. which would you include among your random effects and why? (support your choices with relevant measures, taking into account variance explained and number of parameters going into the modeling) 
    _Subject_ has to be included as a random effect as it represents sub-groups that significantly influence the outcome and make the data non-independent. We could also include _pas_ to be a random slope, as each participant's reaction times may slope differently based on their certainty, but the model failed to converge. 
    ii. explain in your own words what your chosen models says about response times between the different tasks 
```{r}
summary(model_2.1)
```
    The model shows that reaction times are highest for pairs tasks, with somewhat lower reaction times for singles tasks and quadruplet tasks, p < .05. 
3) Now add _pas_ and its interaction with _task_ to the fixed effects 
```{r}
#When you want to model both main effects and their interactions, use an asterisk (*) between the effects. When you want to model ONLY the interaction, use a colon (:).
model_2.3 <- lmerTest::lmer(rt.obj ~ pas * task + (1 | subject), data = exp, REML = FALSE)
summary(model_2.3)
interaction.plot(exp$pas, exp$task, exp$rt.obj) #plot the interaction to help me understand it better
```
    The more complex model doesn't appear to be better than the more simple one: only some of the fixed effects and none of the interaction effects are similar.
    i. how many types of group intercepts (random effects) can you add without ending up with convergence issues or singular fits?
```{r}
model_2.4 <- lmerTest::lmer(rt.obj ~ task + (1 | subject) + (1 | cue) + (1 | trial) + (1 | target.type) + (1 | pas), data = exp, REML = FALSE)
model_2.5 <- lmerTest::lmer(rt.obj ~ task + (1 | subject) + (1 | cue) + (1 | trial) + (1 | target.type) + (1 | pas) + (1 | task), data = exp, REML = FALSE)
```
    We succeeded at adding 5 group intercepts before convergence issues happened.
    ii. create a model by adding random intercepts (without modelling slopes) that results in a singular fit - then use `print(VarCorr(<your.model>), comp='Variance')` to inspect the variance vector - explain why the fit is singular (Hint: read the first paragraph under details in the help for `isSingular`)
```{r}
# I take the model with convergence issues from the previous exercise
print(VarCorr(model_2.5), comp = 'Variance')
```
    iii. in your own words - how could you explain why your model would result in a singular fit?
    When a group intercept is modeled, it is because the grouping variable accounts for some variance in the data - e.g. in within-subject experiments, participants will generally have their own baseline for performance. In other words, by modeling a separate regression line for the data points belonging to the group, there is less distance between the points and the regression line for the group (i.e. less error). The extent of this 'improvement' can be calculated as the variance explained by the grouping variable.
    This same can be done for all other group effects modeled. However, at some point, the combinations of these groupings will create smaller and smaller subgroups, fitting group effects to fewer and fewer data points, to the extent that almost no error will be present for each line because they pretty much lie _on_ the data points. In this case, the model no longer _summarizes_ the data, but rather _conforms_ to it, i.e. is singular.
    
## Exercise 3

1) Initialise a new data frame, `data.count`. _count_ should indicate the number of times they categorized their experience as _pas_ 1-4 for each _task_. I.e. the data frame would have for subject 1: for task:singles, pas1 was used # times, pas2 was used # times, pas3 was used # times and pas4 was used # times. You would then do the same for task:pairs and task:quadruplet  

```{r}
data.count <- df %>%
  group_by(subject, task, pas) %>% 
  summarise('count' = n()) #create grouping data frame

data.count$subject <- as.factor(data.count$subject)
data.count$task <- as.factor(data.count$task)
data.count$pas <- as.factor(data.count$pas)
data.count$count <- as.integer(data.count$count) #ensure correct data format
```        

2) Now fit a multilevel model that models a unique "slope" for _pas_ for each _subject_ with the interaction between _pas_ and _task_ and their main effects being modelled 
```{r}
model_3.2 <- glmer(count ~ pas + task + pas:task + (1 + pas | subject), 
                   data = data.count, 
                   family = 'poisson', 
                   control = glmerControl(optimizer = "bobyqa"))
summary(model_3.2)
```
    i. which family should be used? 
    A Poisson-distribution should be used as we are investigating count data for specific units of measurement.
    HOWEVER:
```{r}
mean(data.count$count)
var(data.count$count)
```
    The Poisson-distribution assumes that the mean and the variance of the count variable is equal. If the variance is higher than the mean, then it is a case of over-dispersion and a quasi-Poisson distribution or a negative binomial distribution is recommended. But we have learned none of this so far so I am staying with the Poisson.
    ii. why is a slope for _pas_ not really being modeled?
    _pas_ is encoded as factor, i.e. a categorical variable, meaning that it is assumed not to lie along a continuum and is therefore analyzed separately per level.
    iii. if you get a convergence error, try another algorithm (the default is the _Nelder_Mead_) - try (_bobyqa_) for which the `dfoptim` package is needed. In `glmer`, you can add the following for the `control` argument: `glmerControl(optimizer="bobyqa")` (if you are interested, also have a look at the function `allFit`)
    iv. when you have a converging fit - fit a model with only the main effects of _pas_ and _task_. Compare this with the model that also includes the interaction 
```{r}
model_3.2b <- glmer(count ~ pas + task + (1 + pas | subject), 
                    data = data.count, 
                    family = 'poisson', 
                    control = glmerControl(optimizer = "bobyqa"))
summary(model_3.2b)
```
    v. indicate which of the two models, you would choose and why 
```{r}
# AIC comparison:
AIC(model_3.2, model_3.2b)

# Residual variance (using formula):
c(var(residuals(model_3.2)),
  var(residuals(model_3.2b))
)

# Residual standard deviation (using formula):
c(sd(residuals(model_3.2)),
  sd(residuals(model_3.2b))
)
```
    When checking AIC scores, residual variance and standard deviation, the model _with_ interactions appears better.
    vi. based on your chosen model - write a short report on what this says about the distribution of ratings as dependent on _pas_ and _task_  
```{r}
#When you want to interpret the output of a Poisson regression, you have to take the exp() of output. In order to not have a mental breakdown, let's collect it into a handy table:
normal <- fixef(model_3.2)
exp <- exp(fixef(model_3.2))
poisson <- as.data.frame(cbind(normal, exp))

#If the value is negative, it implies a decrease, and the exponentiated value has to be subtracted from 1
poisson$corrected <- ifelse((poisson$normal < 0), 1 - poisson$exp, poisson$exp)
poisson$direction <- ifelse((poisson$normal < 0), "decrease", "increase")

poisson
```
    As a baseline, i.e. for when the PAS rating is "1" and the task is "pairs", there are ~56.58 instances. It appears that there is an increasingly smaller probability for participants to rate a trial as a PAS 2 (~2.34%), 3 (~40.01%) or 4 (~53.83%) compared to a PAS 1. Participants also appear less likely to encounter trials with a "singles" task (~20.62%) and more likely to encounter trials with a "quadruplets" task (~12.18%) compared to a "pairs" task.
    An increase in probability for so rating a trial is shown for all interactions moving along the PAS scale for "singles" tasks (~21.57%, ~27.51% and ~75.67%, respectively), while a decrease can be noted for "quadruplets" tasks (~10.75%, ~18.85%, ~19.35% respectively).
    All main and interaction effects are significant, p < .05, with the exception of moving from PAS rating 1 to PAS rating 2.
    vii. include a plot that shows the estimated amount of ratings for four subjects of your choosing.
```{r}
# When making predictions from a regression model, we can make either in-sample or out-of-sample predictions.
# When making in-sample predictions, we basically obtain the predicted values for the y axis based on the existing values on the x axis; i.e. the value on y if there was no error. These are the fitted values.
# Since we are trying to estimate ratings for subjects already in the dataset, we make in-sample predictions.

#Extract the fitted values
data.count$fitted <- fitted(model_3.2)

#Select subjects
df_3.2 <- data.count %>% 
  filter(subject == '1' | subject == '5' | subject == '16' | subject == '20')

ggplot(data = df_3.2, aes(x = pas, y = fitted, fill = task)) +
  geom_col(width = 0.75) +
  facet_wrap(~subject) +
  labs(title = "Predicted amount of PAS ratings per task for 4 subjects", x = "PAS rating", y = "Number of ratings") +
  scale_fill_manual(labels = c("Pairs", "Quadruplets", "Singles"), values = c("aquamarine3", "cornflowerblue", "coral1")) +
  guides(color = guide_legend("Task")) + 
  theme_minimal()
```
3) Finally, fit a multilevel model that models _correct_ as dependent on _task_ with a unique intercept for each _subject_
```{r}
model_3.3a <- glmer(correct ~ task + (1 | subject), family = 'binomial', data = df)
summary(model_3.3a)
```
    i. does _task_ explain performance?  
    ii. add _pas_ as a main effect on top of _task_ - what are the consequences of that?  
```{r}
model_3.3b <- glmer(correct ~ task + pas + (1 | subject), family = 'binomial', data = df)
summary(model_3.3b)
```
    iii. now fit a multilevel model that models _correct_ as dependent on _pas_ with a unique intercept for each _subject_
```{r}
model_3.3c <- glmer(correct ~ pas + (1 | subject), family = 'binomial', data = df)
summary(model_3.3c)
```
    iv. finally, fit a model that models the interaction between _task_ and _pas_  and their main effects  
```{r}
model_3.3d <- glmer(correct ~ task * pas + (1 | subject), family = 'binomial', data = df)
summary(model_3.3d)
```
    v. describe in your words which model is the best in explaining the variance in accuracy  
```{r}
#Residual variance:
c(var(residuals(model_3.3a)),
  var(residuals(model_3.3b)),
  var(residuals(model_3.3c)),
  var(residuals(model_3.3d))
)

#AIC
anova(model_3.3a, model_3.3b, model_3.3c, model_3.3d, test = 'LR')
```
_model_3.3c_ appears to be the most suitable model. It has the lowest residual variance and AIC, and the addition of multiple main effects or of interaction effects did not significantly improve the model's performance. PAS seems to account for most of the variance in the dataset.  

